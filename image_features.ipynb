{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Features\n",
    "*Last updated: 18 Nov 2019*\n",
    "\n",
    "Extracting image features from the CIFAR-10 dataset. Image feature extraction, or image descriptors, is an area of computer vision. Image descriptors include the likes of SIFT, KAZE, HOG, and SIFT. They can be used to classify images by similarity between descriptors. Since the number of descriptors per image will vary depending on the image, you can't directly compare descriptor 1 from image A to descriptor 1 from image B. Instead a \"clustering\" like strategy must be employed to allow comparisons (regression or classification). One common method to do this would be bag of visual words with K-means clustering or K-nearest neighbor clustering.\n",
    "\n",
    "From a search of the web I found that you can use HOG features directly from scikit-image Python module, without the need of BOVW. I need to look more into what these features mean, and how we can interpret them. An example of this on the CIFAR-10 data can be found [here](https://github.com/jvizcar/BACH/blob/master/pipeline.py).\n",
    "\n",
    "This notebook should create the features for all images and add them to the csv file as a feature column. For development purposes the feature columns will be named with the type of features included, in case we test a different set of features. In order to allow list of features to be stored in a dataframe cell and loaded back as a non-string we save pkl representations of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules import utils\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.feature import hog\n",
    "import numpy as np\n",
    "import cv2\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "import random\n",
    "from sklearn.svm import LinearSVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <center>HOG Features</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract features from training data and add as feature column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract features for all images in training dataset and add to csv\n",
    "feature_limit = None\n",
    "train_features = []\n",
    "train_images = np.load('CIFAR10_Data/train.npy')\n",
    "df_train = pd.read_csv('CIFAR10_Data/train.csv')\n",
    "\n",
    "for i, r in tqdm(df_train.iterrows(), total=len(df_train)):\n",
    "    train_features.append(utils.extract_features(train_images[i, :, :, :], 'hog', limit=feature_limit))\n",
    "\n",
    "# add the feature column\n",
    "df_train['hog_features'] = train_features\n",
    "df_train.to_pickle('CIFAR10_Data/train.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract features from testing dta and add as feature column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_limit = None\n",
    "test_features = []\n",
    "test_images = np.load('CIFAR10_Data/test.npy')\n",
    "df_test = pd.read_csv('CIFAR10_Data/test.csv')\n",
    "\n",
    "for i, r in tqdm(df_test.iterrows(), total=len(df_test)):\n",
    "    test_features.append(utils.extract_features(test_images[i, :, :, :], 'hog', limit=feature_limit))\n",
    "\n",
    "# add the feature column\n",
    "df_test['hog_features'] = test_features\n",
    "df_test.to_pickle('CIFAR10_Data/test.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sanity check visualization of the HOG features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 10\n",
    "fig, ax = plt.subplots(ncols=n, nrows=2, figsize=(15,3))\n",
    "ax = ax.ravel()\n",
    "\n",
    "# randomly sample n images from training data and visualize their HOG features (bottom row)\n",
    "for i, idx in enumerate(random.sample(range(0, len(train_images)), n)):\n",
    "    image = train_images[idx, :, :, :]\n",
    "    ax[i].imshow(image, interpolation='bicubic')\n",
    "    ax[i].set_title(df_train.loc[idx, 'label_name'], fontsize=12)\n",
    "    ax[i].axis('off')\n",
    "    \n",
    "    image = cv2.cvtColor(image,cv2.COLOR_BGR2GRAY)\n",
    "    fd, hog_image = hog(image, visualize=True, orientations= 9, pixels_per_cell=[8,8], \n",
    "                    cells_per_block=[2,2], block_norm='L2-Hys', transform_sqrt=True)\n",
    "    ax[i+n].imshow(hog_image, cmap='gray')\n",
    "    ax[i+n].axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sanity check that these features can be used to predict better than chance using support vector machine (SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_classifier(train_path, test_path, feature, subset=None):\n",
    "    \"\"\"Simple SVM classification training and testing score for sanity check that an extracted feature set\n",
    "    is predictive of the data beyond chance. \n",
    "    \n",
    "    :param train_path : str\n",
    "        path to pickle file containing the labels and feature data\n",
    "    :param test_path : str\n",
    "        path to pickle file containing the testing data\n",
    "    :param feature : str\n",
    "        the name of the column containing the features\n",
    "    \n",
    "    :return score : float\n",
    "        testing accuracy on the testing data\n",
    "    \"\"\"\n",
    "    # load data\n",
    "    train_data = pd.read_pickle(train_path)\n",
    "    test_data = pd.read_pickle(test_path)\n",
    "    \n",
    "    if subset is not None:\n",
    "        train_data = train_data[train_data['label'].isin(subset)]\n",
    "        test_data = test_data[test_data['label'].isin(subset)]\n",
    "    \n",
    "    train_X = np.array(list(train_data[feature]))\n",
    "    train_y = np.array(train_data['label'])\n",
    "    \n",
    "    test_X = np.array(list(test_data[feature]))\n",
    "    test_y = np.array(test_data['label'])\n",
    "\n",
    "    # use scikit-learn to train and predict on testing data with simple linear SVM\n",
    "    C = 1.0\n",
    "    clf = LinearSVC(C=C)\n",
    "    clf.fit(train_X, train_y)\n",
    "    score = clf.score(test_X, test_y)\n",
    "    return score\n",
    "\n",
    "\n",
    "score = simple_classifier('CIFAR10_Data/train.pkl', 'CIFAR10_Data/test.pkl', 'hog_features', subset=[0, 5])\n",
    "print('Testing accuracy using HOG features: %.2f' % (score * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the pickled data (dict format) for 2 classes and test SVM for sanity check that everything\n",
    "# is alright\n",
    "import pickle\n",
    "\n",
    "with open('CIFAR10_Data/train_2class.pkl', 'r') as fp:\n",
    "    df = pickle.load(fp)\n",
    "\n",
    "with open('CIFAR10_Data/test_2class.pkl', 'r') as fp:\n",
    "    df_test = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert features to array format for use with sklearn\n",
    "import numpy as np\n",
    "X_train = []\n",
    "X_test = []\n",
    "\n",
    "for k, v in df['hog_features'].items():\n",
    "    X_train.append([v[i] for i in range(len(v))])\n",
    "    \n",
    "for k, v in df_test['hog_features'].items():\n",
    "    X_test.append([v[i] for i in range(len(v))])\n",
    "    \n",
    "train_X = np.array(X_train)\n",
    "test_X = np.array(X_test)\n",
    "train_y = np.array(df['label'])\n",
    "test_y = np.array(df_test['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.935"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C = 1.0\n",
    "clf = LinearSVC(C=C)\n",
    "clf.fit(train_X, train_y)\n",
    "score = clf.score(test_X, test_y)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "clf = MLPClassifier(solver='lbfgs', alpha=1e-5, random_state=1)\n",
    "clf.fit(train_X, train_y)\n",
    "clf.score(test_X, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
