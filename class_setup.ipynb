{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class Setup\n",
    "*Last updated: 21 Nov 2019*\n",
    "\n",
    "Develop the class setup for our approach. This will be analogous to the class setup used CS557 Projects/Homeworks and should have access to the same methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules import utils\n",
    "from modules import util\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from random import shuffle\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>label</th>\n",
       "      <th>label_name</th>\n",
       "      <th>batch</th>\n",
       "      <th>hog_features</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>27</td>\n",
       "      <td>5</td>\n",
       "      <td>dog</td>\n",
       "      <td>1</td>\n",
       "      <td>{0: 0.13396336564598635, 1: 0.1906655101981496...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>airplane</td>\n",
       "      <td>1</td>\n",
       "      <td>{0: 0.0083773331636311, 1: 0.00253145519529918...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>airplane</td>\n",
       "      <td>1</td>\n",
       "      <td>{0: 0.04740756179018544, 1: 0.0339916867737869...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>airplane</td>\n",
       "      <td>1</td>\n",
       "      <td>{0: 0.0077338554316584976, 1: 0.00301717145340...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>40</td>\n",
       "      <td>5</td>\n",
       "      <td>dog</td>\n",
       "      <td>1</td>\n",
       "      <td>{0: 0.07158417861526833, 1: 0.0654108914948450...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index  label label_name  batch  \\\n",
       "0     27      5        dog      1   \n",
       "1     29      0   airplane      1   \n",
       "2     30      0   airplane      1   \n",
       "3     35      0   airplane      1   \n",
       "4     40      5        dog      1   \n",
       "\n",
       "                                        hog_features  \n",
       "0  {0: 0.13396336564598635, 1: 0.1906655101981496...  \n",
       "1  {0: 0.0083773331636311, 1: 0.00253145519529918...  \n",
       "2  {0: 0.04740756179018544, 1: 0.0339916867737869...  \n",
       "3  {0: 0.0077338554316584976, 1: 0.00301717145340...  \n",
       "4  {0: 0.07158417861526833, 1: 0.0654108914948450...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load training data and testing data\n",
    "with open('CIFAR10_Data/train.pkl', 'rb') as fp:\n",
    "    df = pickle.load(fp)\n",
    "    \n",
    "# subset to only dog and airplane classes for developement\n",
    "df = df[df['label_name'].isin(('dog', 'airplane'))].reset_index()\n",
    "\n",
    "# do a conversion of the hog features to dict format\n",
    "for i, r in df.iterrows():\n",
    "    df.at[i, 'hog_features'] = {j: ft for j, ft in enumerate(r['hog_features'])}\n",
    "    \n",
    "# load training data\n",
    "with open('CIFAR10_Data/test.pkl', 'rb') as fp:\n",
    "    df_test = pickle.load(fp)\n",
    "    \n",
    "# subset to only dog and airplane classes for developement\n",
    "df_test = df_test[df_test['label_name'].isin(('dog', 'airplane'))].reset_index()\n",
    "\n",
    "# do a conversion of the hog features to dict format\n",
    "for i, r in df_test.iterrows():\n",
    "    df_test.at[i, 'hog_features'] = {j: ft for j, ft in enumerate(r['hog_features'])}\n",
    "    \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save modified pickles\n",
    "with open('CIFAR10_Data/train_2class.pkl', 'w') as fp:\n",
    "    pickle.dump(df, fp)\n",
    "    \n",
    "with open('CIFAR10_Data/test_2class.pkl', 'w') as fp:\n",
    "    pickle.dump(df_test, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modifying Approximate Q Agent\n",
    "\n",
    "Original code was set to work with Pacman environment. Our version strips this down to only what is needed. \n",
    "\n",
    "Simplifications -\n",
    "* feature extractor returns the image features (i.e. HOG features) for any given state, where a state in our project is a figure in the training dataset\n",
    "* legal actions in this case is the same for any given state, it is just the classes we can predict. Predicting a class is a legal action, and since we can predict any class for any given state (image) it is the same for all\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 of 10\n",
      "\tepoch accuracy: 0.5\n",
      "Epoch 2 of 10\n",
      "\tepoch accuracy: 0.5\n",
      "Epoch 3 of 10\n",
      "\tepoch accuracy: 0.5\n",
      "Epoch 4 of 10\n",
      "\tepoch accuracy: 0.5\n",
      "Epoch 5 of 10\n",
      "\tepoch accuracy: 0.5\n",
      "Epoch 6 of 10\n",
      "\tepoch accuracy: 0.5\n",
      "Epoch 7 of 10\n",
      "\tepoch accuracy: 0.5\n",
      "Epoch 8 of 10\n",
      "\tepoch accuracy: 0.5\n",
      "Epoch 9 of 10\n",
      "\tepoch accuracy: 0.5\n",
      "Epoch 10 of 10\n",
      "\tepoch accuracy: 0.5\n"
     ]
    }
   ],
   "source": [
    "class Feature_Extractor():\n",
    "    \"\"\"Object for extracting features from an image file. The data is provided in a dataframe variable\n",
    "    with a label_names column and an index column for state name.\n",
    "    \"\"\"\n",
    "    def __init__(self, df):\n",
    "        self.states = list(df['index'])\n",
    "        self.features = list(df['hog_features'])\n",
    "        \n",
    "    def getFeatures(self, state, action):\n",
    "        return self.features[state]\n",
    "\n",
    "\n",
    "class QLearningClassifier():\n",
    "    \"\"\"Agent to use for project. Should be a modified version of ApproximateQAgent from Project/Homework 4\n",
    "    in CS557.\n",
    "    \"\"\"\n",
    "    def __init__(self, df, epsilon=0.05, gamma=0.8, alpha=0.2):\n",
    "        # feature extractor object has a method getFeatures(..) that provides a set of features for any given\n",
    "        # state (i.e. image). It takes an action but this does not affect the featurs returned\n",
    "        self.featExtractor = Feature_Extractor(df) \n",
    "        \n",
    "        # alpha    - learning rate\n",
    "        # epsilon  - exploration rate (Not sure what this is, this the random action factor?!?)\n",
    "        # gamma    - discount factor\n",
    "        # numTraining - number of training episodes, i.e. no learning after these many episodes\n",
    "        self.epsilon = float(epsilon)\n",
    "        self.alpha = float(alpha)\n",
    "        self.weights = utils.Counter()\n",
    "        self.labels = list(df['label_name'])\n",
    "        self.legalActions = list(set(self.labels))\n",
    "        self.discount = float(gamma)\n",
    "        \n",
    "    def getWeights(self):\n",
    "        return self.weights\n",
    "    \n",
    "    def getLabel(self, state):\n",
    "        return self.labels[state]\n",
    "    \n",
    "    def computeActionFromQValues(self, state):\n",
    "        \"\"\"\n",
    "          Compute the best action to take in a state.  Note that if there\n",
    "          are no legal actions, which is the case at the terminal state,\n",
    "          you should return None.\n",
    "        \"\"\"\n",
    "        \"*** YOUR CODE HERE ***\"\n",
    "        Q_Values = util.Counter()\n",
    "        actions = self.legalActions\n",
    "        for a in actions:\n",
    "            Q_Values[a] = self.getQValue(state, a)\n",
    "\n",
    "        # Best action (maximizes Q-Value)\n",
    "        max_action = Q_Values.argMax()\n",
    "        return max_action\n",
    "    \n",
    "    def getAction(self, state):\n",
    "        \"\"\"\n",
    "          Compute the action to take in the current state.  With\n",
    "          probability self.epsilon, we should take a random action and\n",
    "          take the best policy action otherwise.  Note that if there are\n",
    "          no legal actions, which is the case at the terminal state, you\n",
    "          should choose None as the action.\n",
    "\n",
    "          HINT: You might want to use util.flipCoin(prob)\n",
    "          HINT: To pick randomly from a list, use random.choice(list)\n",
    "        \"\"\"\n",
    "        # Pick Action\n",
    "        legalActions = self.legalActions\n",
    "        action = self.computeActionFromQValues(state)\n",
    "        return action\n",
    "    \n",
    "    def getQValue(self, state, action):\n",
    "        \"\"\"For a given state, action pair it should return the dot product of the weight vector and the\n",
    "        feature vector for that state. In our case the feature vector is the image descriptors for that image\n",
    "        (note that an image is a state in our project).\n",
    "          Should return Q(state,action) = w * featureVector\n",
    "          where * is the dotProduct operator\n",
    "        \"\"\"\n",
    "        # Q(state, action) = w dot featureVector\n",
    "        features = self.featExtractor.getFeatures(state, action)\n",
    "        QValue = sum( self.weights[i] * features[i] for i in features.keys() )\n",
    "        return QValue\n",
    "            \n",
    "    def update(self, state, action, nextState, reward):\n",
    "        \"\"\"\n",
    "           Should update your weights based on transition\n",
    "        \"\"\"\n",
    "        \"*** YOUR CODE HERE ***\"\n",
    "        # Q-Values for each a' :\n",
    "        Q_Counter = util.Counter()\n",
    "\n",
    "        # legal actions is trivial in our case as there are always as many actions as classes\n",
    "        a_prime_values = self.legalActions\n",
    "        for a_prime in a_prime_values:\n",
    "            # Q-Value for a':\n",
    "            Q_Counter[a_prime] = self.getQValue(nextState, a_prime)\n",
    "\n",
    "        # difference = (R + gamma * max[Q(s',a')] ) - Q(s,a)\n",
    "        difference = (reward + self.discount * Q_Counter[Q_Counter.argMax()] ) - self.getQValue(state, action)\n",
    "\n",
    "        # wi = wi + alpha * difference * fi(s,a)\n",
    "        features = self.featExtractor.getFeatures(state, action)\n",
    "        for i in features.keys():\n",
    "            self.weights[i] = self.weights[i] + self.alpha * difference * features[i]\n",
    "        return 0  \n",
    "    \n",
    "    def train(self, epochs=10):\n",
    "        \"\"\"Run through the training using the trainign dataset.\n",
    "        \"\"\"\n",
    "        \n",
    "        for i, e in enumerate(range(epochs)):\n",
    "            print('Epoch {} of {}'.format(i+1, epochs))\n",
    "            state_list = list(range(len(self.labels)))\n",
    "            shuffle(state_list)\n",
    "            \n",
    "            # get the initial state\n",
    "            state = state_list.pop()\n",
    "            while len(state_list) > 0:\n",
    "                # get the best action for this state by Q-values\n",
    "                action = self.getAction(state)\n",
    "                \n",
    "                nextState = state_list.pop()\n",
    "                \n",
    "                # check if action matches label, if true then reward is 1 else it is 0\n",
    "                \n",
    "                if action == self.labels[state]:\n",
    "                    reward = 10\n",
    "                else:\n",
    "                    reward = -10\n",
    "                \n",
    "                self.update(state, action, nextState, reward)\n",
    "                \n",
    "            acc = game.test(df_test)\n",
    "            print('\\tepoch accuracy: {}'.format(acc))\n",
    "                \n",
    "    def test(self, test_data):\n",
    "        legalActions = self.legalActions\n",
    "        \n",
    "        # report testing accuracy\n",
    "        featExtractor = Feature_Extractor(test_data) \n",
    "        labels = list(test_data['label_name'])\n",
    "        \n",
    "        correct_count = 0\n",
    "        actions = self.legalActions\n",
    "        \n",
    "        state_list = list(range(len(labels)))\n",
    "                \n",
    "        for state in state_list:\n",
    "            Q_Values = util.Counter()\n",
    "            for a in actions:\n",
    "                features = featExtractor.getFeatures(state, a)\n",
    "                QValue = sum( self.weights[i] * features[i] for i in features.keys() )\n",
    "                Q_Values[a] = QValue\n",
    "\n",
    "            # Best action (maximizes Q-Value)\n",
    "            max_action = Q_Values.argMax()\n",
    "            \n",
    "            if max_action == labels[state]:\n",
    "                correct_count += 1\n",
    "                \n",
    "        accuracy = float(correct_count) / float(len(state_list))\n",
    "        return accuracy\n",
    "        \n",
    "        \n",
    "game = QLearningClassifier(df)  \n",
    "game.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "game.weights"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
