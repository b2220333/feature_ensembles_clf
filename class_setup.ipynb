{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class Setup\n",
    "*Last updated: 21 Nov 2019*\n",
    "\n",
    "Develop the class setup for our approach. This will be analogous to the class setup used CS557 Projects/Homeworks and should have access to the same methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules import utils\n",
    "import pandas as pd\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>label</th>\n",
       "      <th>label_name</th>\n",
       "      <th>batch</th>\n",
       "      <th>hog_features</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>27</td>\n",
       "      <td>5</td>\n",
       "      <td>dog</td>\n",
       "      <td>1</td>\n",
       "      <td>{0: 0.13396336564598635, 1: 0.1906655101981496...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>airplane</td>\n",
       "      <td>1</td>\n",
       "      <td>{0: 0.0083773331636311, 1: 0.00253145519529918...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>airplane</td>\n",
       "      <td>1</td>\n",
       "      <td>{0: 0.04740756179018544, 1: 0.0339916867737869...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>airplane</td>\n",
       "      <td>1</td>\n",
       "      <td>{0: 0.0077338554316584976, 1: 0.00301717145340...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>40</td>\n",
       "      <td>5</td>\n",
       "      <td>dog</td>\n",
       "      <td>1</td>\n",
       "      <td>{0: 0.07158417861526833, 1: 0.0654108914948450...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index  label label_name  batch  \\\n",
       "0     27      5        dog      1   \n",
       "1     29      0   airplane      1   \n",
       "2     30      0   airplane      1   \n",
       "3     35      0   airplane      1   \n",
       "4     40      5        dog      1   \n",
       "\n",
       "                                        hog_features  \n",
       "0  {0: 0.13396336564598635, 1: 0.1906655101981496...  \n",
       "1  {0: 0.0083773331636311, 1: 0.00253145519529918...  \n",
       "2  {0: 0.04740756179018544, 1: 0.0339916867737869...  \n",
       "3  {0: 0.0077338554316584976, 1: 0.00301717145340...  \n",
       "4  {0: 0.07158417861526833, 1: 0.0654108914948450...  "
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load training data\n",
    "with open('CIFAR10_Data/train.pkl', 'rb') as fp:\n",
    "    df = pickle.load(fp)\n",
    "    \n",
    "# subset to only dog and airplane classes for developement\n",
    "df = df[df['label_name'].isin(('dog', 'airplane'))].reset_index()\n",
    "\n",
    "# do a conversion of the hog features to dict format\n",
    "for i, r in df.iterrows():\n",
    "    df.at[i, 'hog_features'] = {j: ft for j, ft in enumerate(r['hog_features'])}\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modifying Approximate Q Agent\n",
    "\n",
    "Original code was set to work with Pacman environment. Our version strips this down to only what is needed. \n",
    "\n",
    "Simplifications -\n",
    "* feature extractor returns the image features (i.e. HOG features) for any given state, where a state in our project is a figure in the training dataset\n",
    "* legal actions in this case is the same for any given state, it is just the classes we can predict. Predicting a class is a legal action, and since we can predict any class for any given state (image) it is the same for all\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Feature_Extractor():\n",
    "    \"\"\"Object for extracting features from an image file. The data is provided in a dataframe variable\n",
    "    with a label_names column and an index column for state name.\n",
    "    \"\"\"\n",
    "    def __init__(self, df):\n",
    "        self.states = list(df['index'])\n",
    "        self.features = list(df['hog_features'])\n",
    "        \n",
    "    def get_features(self, state, action):\n",
    "        return self.features[state]\n",
    "\n",
    "\n",
    "class Agent():\n",
    "    \"\"\"Agent to use for project. Should be a modified version of ApproximateQAgent from Project/Homework 4\n",
    "    in CS557.\n",
    "    \"\"\"\n",
    "    def __init__(self, df, epsilon=0.05, gamma=0.8, alpha=0.2):\n",
    "        # feature extractor object has a method getFeatures(..) that provides a set of features for any given\n",
    "        # state (i.e. image). It takes an action but this does not affect the featurs returned\n",
    "        self.featExtractor = Feature_Extractor(df) \n",
    "        \n",
    "        # alpha    - learning rate\n",
    "        # epsilon  - exploration rate (Not sure what this is, this the random action factor?!?)\n",
    "        # gamma    - discount factor\n",
    "        # numTraining - number of training episodes, i.e. no learning after these many episodes\n",
    "        self.epsilon = epsilon\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "        self.weights = utils.Counter()\n",
    "        self.labels = list(df['label_name'])\n",
    "        self.legalActions = list(set(self.labels))\n",
    "        \n",
    "    def getWeights(self):\n",
    "        return self.weights\n",
    "    \n",
    "    def getLabel(self, state):\n",
    "        return self.labels[state]\n",
    "    \n",
    "    def getQValue(self, state, action):\n",
    "        \"\"\"For a given state, action pair it should return the dot product of the weight vector and the\n",
    "        feature vector for that state. In our case the feature vector is the image descriptors for that image\n",
    "        (note that an image is a state in our project).\n",
    "          Should return Q(state,action) = w * featureVector\n",
    "          where * is the dotProduct operator\n",
    "        \"\"\"\n",
    "        # Q(state, action) = w dot featureVector\n",
    "        features = self.featExtractor.getFeatures(state, action)\n",
    "        QValue = sum( self.weights[i] * features[i] for i in features.keys() )\n",
    "        return QValue\n",
    "            \n",
    "    def update(self, state, action, nextState, reward):\n",
    "        \"\"\"\n",
    "           Should update your weights based on transition\n",
    "        \"\"\"\n",
    "        \"*** YOUR CODE HERE ***\"\n",
    "        # Q-Values for each a' :\n",
    "        Q_Counter = util.Counter()\n",
    "\n",
    "        # legal actions is trivial in our case as there are always as many actions as classes\n",
    "        a_prime_values = self.legalActions\n",
    "        for a_prime in a_prime_values:\n",
    "            # Q-Value for a':\n",
    "            Q_Counter[a_prime] = self.getQValue(nextState, a_prime)\n",
    "\n",
    "        # difference = (R + gamma * max[Q(s',a')] ) - Q(s,a)\n",
    "        difference = (reward + self.discount * Q_Counter[Q_Counter.argMax()] ) - self.getQValue(state, action)\n",
    "\n",
    "        # wi = wi + alpha * difference * fi(s,a)\n",
    "        features = self.featExtractor.getFeatures(state, action)\n",
    "        for i in features.keys():\n",
    "            self.weights[i] = self.weights[i] + self.alpha * difference * features[i]\n",
    "        return 0  \n",
    "    \n",
    "    # what is needed --\n",
    "    # how to calculate reward? Set to 1 if the predicted class was correct, otherwise it is 0 (or -1?)\n",
    "    \"\"\"FELIPE - how would we calculate a reward here?\n",
    "    \n",
    "    Would reward be given by a state, action pair as 1 if the action (i.e. dog) matches the label and 0\n",
    "    otherwise?\n",
    "    \n",
    "    What about transitions? This is not anywhere in this approach...\"\"\"\n",
    "        \n",
    "        \n",
    "test_agent = Agent(df)  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
